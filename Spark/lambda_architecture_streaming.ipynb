{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d37af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691fb65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/03 15:57:20 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/04/03 15:57:20 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# %fs   - for using in DataBricks, \"spark\" already runned\n",
    "# ls /databricks-datasets\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"lambda\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0626e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_df = spark.read\\\n",
    "   .option(\"header\", True)\\\n",
    "   .option(\"inferSchema\", True)\\\n",
    "   .csv(\"/datalake/bronze/dshop/orders/spark_stream/orders_2022-02-24.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bf8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = [i for i in orders_df.schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b719dad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(order_id,IntegerType,true),\n",
       " StructField(product_id,IntegerType,true),\n",
       " StructField(client_id,IntegerType,true),\n",
       " StructField(store_id,IntegerType,true),\n",
       " StructField(quantity,IntegerType,true),\n",
       " StructField(order_date,StringType,true)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d22237",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema = StructType(orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0998f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data = spark.readStream\\\n",
    "    .schema(csv_schema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .csv(\"/datalake/bronze/dshop/orders/spark_stream/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4247742",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = orders_data\\\n",
    "    .where(F.col('store_id') == 1)\n",
    "orders_df = orders_df\\\n",
    "    .withColumn('quantity', F.round(F.col('quantity') * 3 / 2, 2))\\\n",
    "    .withColumnRenamed('quantity', 'order_quantity')\\\n",
    "    .withColumn('order_date', F.col('order_date').cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41df675d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: int, product_id: int, client_id: int, store_id: int, order_quantity: double, order_date: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2078e2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:57:36 WARN streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8013bbc6-a9fe-4d90-a48d-6fa38e5e998a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders = orders_df.writeStream\\\n",
    "    .format('memory')\\\n",
    "    .queryName('orders_stream')\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80cbee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders_stream_df = spark.table(\"orders_stream\").where(col('store_id') == 1)\n",
    "# orders_stream_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7b790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+--------+--------------+----------+\n",
      "|order_id|product_id|client_id|store_id|order_quantity|order_date|\n",
      "+--------+----------+---------+--------+--------------+----------+\n",
      "|      17|     22637|      569|       1|           4.5|2021-01-01|\n",
      "|      17|     27577|      569|       1|           6.0|2021-01-01|\n",
      "|      17|      4168|      569|       1|           6.0|2021-01-01|\n",
      "|      17|     19135|      569|       1|           3.0|2021-01-01|\n",
      "|      17|      8650|      569|       1|           9.0|2021-01-01|\n",
      "|      17|     11289|      569|       1|           1.5|2021-01-01|\n",
      "|      17|      2056|      569|       1|           9.0|2021-01-01|\n",
      "|      17|     44526|      569|       1|           1.5|2021-01-01|\n",
      "|      17|     17653|      569|       1|           6.0|2021-01-01|\n",
      "|      17|      5302|      569|       1|           7.5|2021-01-01|\n",
      "|      17|     14680|      569|       1|           3.0|2021-01-01|\n",
      "|      21|     17569|     1130|       1|           7.5|2021-01-01|\n",
      "|      21|     34373|     1130|       1|           9.0|2021-01-01|\n",
      "|      21|     37078|     1130|       1|           4.5|2021-01-01|\n",
      "|      21|     30632|     1130|       1|           1.5|2021-01-01|\n",
      "|      21|     45962|     1130|       1|           3.0|2021-01-01|\n",
      "|      21|     14924|     1130|       1|           1.5|2021-01-01|\n",
      "|      21|      2551|     1130|       1|           6.0|2021-01-01|\n",
      "|      21|     49286|     1130|       1|           4.5|2021-01-01|\n",
      "|      21|     34649|     1130|       1|           7.5|2021-01-01|\n",
      "+--------+----------+---------+--------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:57:43 WARN scheduler.TaskSetManager: Stage 11 contains a task of very large size (8670 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM orders_stream\").show()\n",
    "# %sql\n",
    "# select count(*) from orders_stream where store_id = 1    # DataBricks selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd6ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  136764|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:57:51 WARN scheduler.TaskSetManager: Stage 12 contains a task of very large size (9754 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM orders_stream\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24117703",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7eebc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema = StructType([StructField(\"client_id\", IntegerType(), False), StructField(\"client_name\", StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "670057a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .schema(csv_schema)\\\n",
    "    .csv(\"/datalake/bronze/dshop/clients/2022-03-04/clients.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58fbcd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clients_df = spark.table(\"dim_client\")  # in DataBricks possibly to use \"dim_client\" table from another session without table definition and population "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9ee6898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|client_id|    client_name|\n",
      "+---------+---------------+\n",
      "|        1| Anthony Reilly|\n",
      "|        2|Christina Boyle|\n",
      "|        3|  Andrew Walker|\n",
      "|        4|Emily Rodriguez|\n",
      "|        5|    Glen Travis|\n",
      "|        6|     Adam Mayer|\n",
      "|        7|  Lydia Griffin|\n",
      "|        8|     Marcus Cox|\n",
      "|        9|   Sean Johnson|\n",
      "|       10|    Joel Mullen|\n",
      "|       11|   Laurie Brown|\n",
      "|       12|  Kevin Johnson|\n",
      "|       13|    Paula Lopez|\n",
      "|       14|Michelle Hodges|\n",
      "|       15|   Jerome Russo|\n",
      "|       16|Jonathon Porter|\n",
      "|       17| Andrew Jackson|\n",
      "|       18|   Emma Jackson|\n",
      "|       19|     Pam Wilson|\n",
      "|       20| Andrea Sanders|\n",
      "+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:58:00 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n"
     ]
    }
   ],
   "source": [
    "clients_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6343a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_df = orders_df.join(clients_df, orders_df.client_id == clients_df.client_id, 'inner')\\\n",
    "    .select(\n",
    "         orders_df.order_id,\n",
    "         orders_df.product_id,\n",
    "         clients_df.client_name,\n",
    "         orders_df.order_quantity,\n",
    "         orders_df.order_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fc6421d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: int, product_id: int, client_name: string, order_quantity: double, order_date: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(enriched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27e24380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:58:05 WARN streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2f9df43c-d860-4f15-9b93-411be4a9379e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/04/03 15:58:05 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:06 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:06 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:07 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:07 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:08 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:08 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:09 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:10 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n"
     ]
    }
   ],
   "source": [
    "enriched = enriched_df.writeStream\\\n",
    "    .format('memory')\\\n",
    "    .queryName('enriched_stream')\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f8fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------+----------+\n",
      "|order_id|product_id|         client_name|order_quantity|order_date|\n",
      "+--------+----------+--------------------+--------------+----------+\n",
      "|      17|     22637|       Hannah Cooper|           4.5|2021-01-01|\n",
      "|      17|     27577|       Hannah Cooper|           6.0|2021-01-01|\n",
      "|      17|      4168|       Hannah Cooper|           6.0|2021-01-01|\n",
      "|      17|     19135|       Hannah Cooper|           3.0|2021-01-01|\n",
      "|      17|      8650|       Hannah Cooper|           9.0|2021-01-01|\n",
      "|      17|     11289|       Hannah Cooper|           1.5|2021-01-01|\n",
      "|      17|      2056|       Hannah Cooper|           9.0|2021-01-01|\n",
      "|      17|     44526|       Hannah Cooper|           1.5|2021-01-01|\n",
      "|      17|     17653|       Hannah Cooper|           6.0|2021-01-01|\n",
      "|      17|      5302|       Hannah Cooper|           7.5|2021-01-01|\n",
      "|      17|     14680|       Hannah Cooper|           3.0|2021-01-01|\n",
      "|      21|     17569|       Julie Jackson|           7.5|2021-01-01|\n",
      "|      21|     34373|       Julie Jackson|           9.0|2021-01-01|\n",
      "|      21|     37078|       Julie Jackson|           4.5|2021-01-01|\n",
      "|      21|     30632|       Julie Jackson|           1.5|2021-01-01|\n",
      "|      21|     45962|       Julie Jackson|           3.0|2021-01-01|\n",
      "|      21|     14924|       Julie Jackson|           1.5|2021-01-01|\n",
      "|      21|      2551|       Julie Jackson|           6.0|2021-01-01|\n",
      "|      21|     49286|       Julie Jackson|           4.5|2021-01-01|\n",
      "|      21|     34649|       Julie Jackson|           7.5|2021-01-01|\n",
      "|      21|     43932|       Julie Jackson|           9.0|2021-01-01|\n",
      "|      21|      3894|       Julie Jackson|           9.0|2021-01-01|\n",
      "|      21|     20627|       Julie Jackson|           6.0|2021-01-01|\n",
      "|      21|     46737|       Julie Jackson|           9.0|2021-01-01|\n",
      "|      21|     44008|       Julie Jackson|           7.5|2021-01-01|\n",
      "|      21|     22735|       Julie Jackson|           6.0|2021-01-01|\n",
      "|      21|     11749|       Julie Jackson|           6.0|2021-01-01|\n",
      "|      21|      4736|       Julie Jackson|           3.0|2021-01-01|\n",
      "|      21|     49080|       Julie Jackson|           9.0|2021-01-01|\n",
      "|      21|     32158|       Julie Jackson|           9.0|2021-01-01|\n",
      "|      21|     39855|       Julie Jackson|           6.0|2021-01-01|\n",
      "|      21|     40051|       Julie Jackson|           9.0|2021-01-01|\n",
      "|      21|     35254|       Julie Jackson|           4.5|2021-01-01|\n",
      "|      21|      3795|       Julie Jackson|           7.5|2021-01-01|\n",
      "|      21|     18602|       Julie Jackson|           7.5|2021-01-01|\n",
      "|      21|      9193|       Julie Jackson|           6.0|2021-01-01|\n",
      "|      21|      5371|       Julie Jackson|           1.5|2021-01-01|\n",
      "|      26|     42071|    Stephanie Kelley|           9.0|2021-01-01|\n",
      "|      26|     48755|    Stephanie Kelley|           1.5|2021-01-01|\n",
      "|      26|      6219|    Stephanie Kelley|           7.5|2021-01-01|\n",
      "|      26|      6522|    Stephanie Kelley|           6.0|2021-01-01|\n",
      "|      26|      6336|    Stephanie Kelley|           7.5|2021-01-01|\n",
      "|      26|     31198|    Stephanie Kelley|           6.0|2021-01-01|\n",
      "|      26|     11979|    Stephanie Kelley|           1.5|2021-01-01|\n",
      "|      26|     21941|    Stephanie Kelley|           3.0|2021-01-01|\n",
      "|      26|     10936|    Stephanie Kelley|           3.0|2021-01-01|\n",
      "|      26|     45970|    Stephanie Kelley|           1.5|2021-01-01|\n",
      "|      26|      9183|    Stephanie Kelley|           9.0|2021-01-01|\n",
      "|      26|     24558|    Stephanie Kelley|           1.5|2021-01-01|\n",
      "|      26|     10010|    Stephanie Kelley|           7.5|2021-01-01|\n",
      "|      26|     21537|    Stephanie Kelley|           3.0|2021-01-01|\n",
      "|      26|     10948|    Stephanie Kelley|           7.5|2021-01-01|\n",
      "|      28|     13089|         Larry Jones|           7.5|2021-01-01|\n",
      "|      28|      6065|         Larry Jones|           3.0|2021-01-01|\n",
      "|      28|     14860|         Larry Jones|           7.5|2021-01-01|\n",
      "|      28|       357|         Larry Jones|           1.5|2021-01-01|\n",
      "|      28|      2974|         Larry Jones|           4.5|2021-01-01|\n",
      "|      70|     21864|Mr. Joseph Contre...|           4.5|2021-01-01|\n",
      "|      70|     46368|Mr. Joseph Contre...|           3.0|2021-01-01|\n",
      "|      70|     40403|Mr. Joseph Contre...|           1.5|2021-01-01|\n",
      "|      70|      5411|Mr. Joseph Contre...|           9.0|2021-01-01|\n",
      "|      70|      7339|Mr. Joseph Contre...|           4.5|2021-01-01|\n",
      "|      70|      1152|Mr. Joseph Contre...|           3.0|2021-01-01|\n",
      "|      70|     29081|Mr. Joseph Contre...|           9.0|2021-01-01|\n",
      "|      70|     35939|Mr. Joseph Contre...|           6.0|2021-01-01|\n",
      "|      70|     13335|Mr. Joseph Contre...|           7.5|2021-01-01|\n",
      "|      70|      2684|Mr. Joseph Contre...|           9.0|2021-01-01|\n",
      "|      70|      2933|Mr. Joseph Contre...|           3.0|2021-01-01|\n",
      "|      70|     20550|Mr. Joseph Contre...|           3.0|2021-01-01|\n",
      "|      70|     30464|Mr. Joseph Contre...|           3.0|2021-01-01|\n",
      "|      70|     31102|Mr. Joseph Contre...|           3.0|2021-01-01|\n",
      "|      70|     43157|Mr. Joseph Contre...|           1.5|2021-01-01|\n",
      "|      70|     11985|Mr. Joseph Contre...|           1.5|2021-01-01|\n",
      "|     100|      2084|       Ronald George|           6.0|2021-01-01|\n",
      "|     100|     42888|       Ronald George|           4.5|2021-01-01|\n",
      "|     100|     44633|       Ronald George|           4.5|2021-01-01|\n",
      "|     100|     27037|       Ronald George|           9.0|2021-01-01|\n",
      "|     100|     27031|       Ronald George|           9.0|2021-01-01|\n",
      "|     100|     40974|       Ronald George|           6.0|2021-01-01|\n",
      "|     100|     46561|       Ronald George|           4.5|2021-01-01|\n",
      "|     100|     27801|       Ronald George|           1.5|2021-01-01|\n",
      "|     100|     34767|       Ronald George|           7.5|2021-01-01|\n",
      "|     100|     45450|       Ronald George|           7.5|2021-01-01|\n",
      "|     100|     22600|       Ronald George|           1.5|2021-01-01|\n",
      "|     100|     11712|       Ronald George|           7.5|2021-01-01|\n",
      "|     100|     29348|       Ronald George|           1.5|2021-01-01|\n",
      "|     100|     33094|       Ronald George|           3.0|2021-01-01|\n",
      "|     100|      7926|       Ronald George|           9.0|2021-01-01|\n",
      "|     100|     11321|       Ronald George|           4.5|2021-01-01|\n",
      "|     100|      8072|       Ronald George|           9.0|2021-01-01|\n",
      "|     100|     42209|       Ronald George|           9.0|2021-01-01|\n",
      "|     100|     13105|       Ronald George|           3.0|2021-01-01|\n",
      "|     133|     23336|          Amy Morris|           6.0|2021-01-01|\n",
      "|     133|      7141|          Amy Morris|           3.0|2021-01-01|\n",
      "|     133|      2484|          Amy Morris|           7.5|2021-01-01|\n",
      "|     133|     15297|          Amy Morris|           9.0|2021-01-01|\n",
      "|     133|     33640|          Amy Morris|           9.0|2021-01-01|\n",
      "|     133|     36396|          Amy Morris|           6.0|2021-01-01|\n",
      "|     133|     12151|          Amy Morris|           1.5|2021-01-01|\n",
      "|     133|      8986|          Amy Morris|           3.0|2021-01-01|\n",
      "+--------+----------+--------------------+--------------+----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:58:15 WARN scheduler.TaskSetManager: Stage 33 contains a task of very large size (10880 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM enriched_stream\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47181684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  136764|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:58:20 WARN scheduler.TaskSetManager: Stage 34 contains a task of very large size (10880 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM enriched_stream\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35547d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4054cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % sql\n",
    "# CREATE OR REPLACE TABLE tmp.rules(\n",
    "#    user_gender varchar(2),\n",
    "#    age_less_than int,\n",
    "#    age_greater_than int,\n",
    "#    weight_less_than int,\n",
    "#    weight_greater_than int,\n",
    "#    user_is_smoker varchar(2),\n",
    "#    km_walked_greater_than int\n",
    "# );\n",
    "\n",
    "# INSERT into tmp.rules VALUES ('M', NULL, 52, NULL, 147, 'Y', 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "904821df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select * from tmp.rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "497df603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules_df = spark.table(\"tmp.rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84c85d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema = StructType([\\\n",
    "                        StructField(\"quantity_less_than\", DoubleType(), True)\\\n",
    "                       ,StructField(\"quantity_greater_than\", DoubleType(), True)\\\n",
    "                       ,StructField(\"date_less_than\", DateType(), True)\n",
    "                       ,StructField(\"date_greater_than\", DateType(), True)\\\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2da85f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .schema(csv_schema)\\\n",
    "    .csv(\"/datalake/bronze/samples/rules.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac2c1f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+--------------+-----------------+\n",
      "|quantity_less_than|quantity_greater_than|date_less_than|date_greater_than|\n",
      "+------------------+---------------------+--------------+-----------------+\n",
      "|               9.0|                  2.0|    2021-02-15|       2021-02-01|\n",
      "+------------------+---------------------+--------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[quantity_less_than: double, quantity_greater_than: double, date_less_than: date, date_greater_than: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rules_df.show()\n",
    "display(rules_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b9c316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [\n",
    "    ((enriched_df.order_quantity > rules_df.quantity_greater_than) & (enriched_df.order_quantity < rules_df.quantity_less_than))\n",
    "    & ((enriched_df.order_date > rules_df.date_greater_than) & (enriched_df.order_date < rules_df.date_less_than)) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0d0ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notifications_df = enriched_df.join(rules_df, cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10b6dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_notify_df = notifications_df.select('order_id','product_id','client_name','order_quantity','order_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23d90869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:58:43 WARN streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-440f9402-b642-446f-bcb3-87f7d434a6ee. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/04/03 15:58:43 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:44 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:45 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:45 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:46 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:47 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:47 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:48 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:58:48 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n"
     ]
    }
   ],
   "source": [
    "users_notify = users_notify_df.writeStream\\\n",
    "    .format('memory')\\\n",
    "    .queryName('users_notify_stream')\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7666c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+--------------+----------+\n",
      "|order_id|product_id|    client_name|order_quantity|order_date|\n",
      "+--------+----------+---------------+--------------+----------+\n",
      "|    8150|     16276|    Megan Baker|           7.5|2021-02-02|\n",
      "|    8150|     37617|    Megan Baker|           3.0|2021-02-02|\n",
      "|    8150|     43342|    Megan Baker|           4.5|2021-02-02|\n",
      "|    8150|     36527|    Megan Baker|           3.0|2021-02-02|\n",
      "|    8150|     19601|    Megan Baker|           4.5|2021-02-02|\n",
      "|    8176|     43510|  Andrew Walker|           3.0|2021-02-02|\n",
      "|    8176|     32445|  Andrew Walker|           3.0|2021-02-02|\n",
      "|    8176|     38084|  Andrew Walker|           7.5|2021-02-02|\n",
      "|    8176|     37158|  Andrew Walker|           4.5|2021-02-02|\n",
      "|    8176|     20513|  Andrew Walker|           7.5|2021-02-02|\n",
      "|    8176|     39468|  Andrew Walker|           3.0|2021-02-02|\n",
      "|    8176|     31677|  Andrew Walker|           3.0|2021-02-02|\n",
      "|    8176|     14963|  Andrew Walker|           6.0|2021-02-02|\n",
      "|    8176|     29582|  Andrew Walker|           7.5|2021-02-02|\n",
      "|    8176|      9910|  Andrew Walker|           3.0|2021-02-02|\n",
      "|    8176|     20483|  Andrew Walker|           4.5|2021-02-02|\n",
      "|    8176|     38382|  Andrew Walker|           4.5|2021-02-02|\n",
      "|    8176|      2615|  Andrew Walker|           4.5|2021-02-02|\n",
      "|    8176|     45392|  Andrew Walker|           6.0|2021-02-02|\n",
      "|    8176|     23074|  Andrew Walker|           4.5|2021-02-02|\n",
      "|    8176|     11848|  Andrew Walker|           4.5|2021-02-02|\n",
      "|    8176|     38913|  Andrew Walker|           4.5|2021-02-02|\n",
      "|    8176|     36103|  Andrew Walker|           7.5|2021-02-02|\n",
      "|    8176|     27764|  Andrew Walker|           3.0|2021-02-02|\n",
      "|    8208|     26042|  Angela Wright|           6.0|2021-02-02|\n",
      "|    8208|     37554|  Angela Wright|           3.0|2021-02-02|\n",
      "|    8208|     15521|  Angela Wright|           4.5|2021-02-02|\n",
      "|    8208|     37749|  Angela Wright|           6.0|2021-02-02|\n",
      "|    8208|     25524|  Angela Wright|           3.0|2021-02-02|\n",
      "|    8208|     46882|  Angela Wright|           6.0|2021-02-02|\n",
      "|    8208|     27385|  Angela Wright|           7.5|2021-02-02|\n",
      "|    8292|      1430|      Amy Smith|           7.5|2021-02-02|\n",
      "|    8292|      5588|      Amy Smith|           6.0|2021-02-02|\n",
      "|    8292|     40538|      Amy Smith|           6.0|2021-02-02|\n",
      "|    8292|      3704|      Amy Smith|           7.5|2021-02-02|\n",
      "|    8292|     34360|      Amy Smith|           3.0|2021-02-02|\n",
      "|    8292|     19268|      Amy Smith|           6.0|2021-02-02|\n",
      "|    8292|     43831|      Amy Smith|           4.5|2021-02-02|\n",
      "|    8292|     44810|      Amy Smith|           4.5|2021-02-02|\n",
      "|    8292|     15198|      Amy Smith|           3.0|2021-02-02|\n",
      "|    8292|     11323|      Amy Smith|           7.5|2021-02-02|\n",
      "|    8292|     41594|      Amy Smith|           6.0|2021-02-02|\n",
      "|    8292|     32051|      Amy Smith|           6.0|2021-02-02|\n",
      "|    8292|     22148|      Amy Smith|           6.0|2021-02-02|\n",
      "|    8292|     28012|      Amy Smith|           3.0|2021-02-02|\n",
      "|    8292|     12088|      Amy Smith|           4.5|2021-02-02|\n",
      "|    8292|     15470|      Amy Smith|           3.0|2021-02-02|\n",
      "|    8292|     47839|      Amy Smith|           3.0|2021-02-02|\n",
      "|    8292|     26950|      Amy Smith|           4.5|2021-02-02|\n",
      "|    8292|      1193|      Amy Smith|           7.5|2021-02-02|\n",
      "|    8292|      2323|      Amy Smith|           4.5|2021-02-02|\n",
      "|    8292|     32687|      Amy Smith|           6.0|2021-02-02|\n",
      "|    8292|     26571|      Amy Smith|           3.0|2021-02-02|\n",
      "|    8292|     39514|      Amy Smith|           7.5|2021-02-02|\n",
      "|    8300|     33406|  Rebecca Miles|           4.5|2021-02-03|\n",
      "|    8300|      2973|  Rebecca Miles|           4.5|2021-02-03|\n",
      "|    8300|      5654|  Rebecca Miles|           7.5|2021-02-03|\n",
      "|    8300|     29204|  Rebecca Miles|           6.0|2021-02-03|\n",
      "|    8300|     39818|  Rebecca Miles|           6.0|2021-02-03|\n",
      "|    8300|       299|  Rebecca Miles|           6.0|2021-02-03|\n",
      "|    8300|     14040|  Rebecca Miles|           4.5|2021-02-03|\n",
      "|    8300|     43484|  Rebecca Miles|           3.0|2021-02-03|\n",
      "|    8300|     39709|  Rebecca Miles|           3.0|2021-02-03|\n",
      "|    8300|     29513|  Rebecca Miles|           7.5|2021-02-03|\n",
      "|    8330|       258| Bradley Wilson|           4.5|2021-02-03|\n",
      "|    8330|      7760| Bradley Wilson|           6.0|2021-02-03|\n",
      "|    8330|     37734| Bradley Wilson|           3.0|2021-02-03|\n",
      "|    8365|     20083| Alyssa Richard|           7.5|2021-02-03|\n",
      "|    8365|     15715| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|     22852| Alyssa Richard|           4.5|2021-02-03|\n",
      "|    8365|     41031| Alyssa Richard|           7.5|2021-02-03|\n",
      "|    8365|     46561| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|     48486| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|      6775| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|     38790| Alyssa Richard|           7.5|2021-02-03|\n",
      "|    8365|      7728| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|     21686| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|     32251| Alyssa Richard|           4.5|2021-02-03|\n",
      "|    8365|     37036| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|     40119| Alyssa Richard|           7.5|2021-02-03|\n",
      "|    8365|       281| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|     32417| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|     30645| Alyssa Richard|           4.5|2021-02-03|\n",
      "|    8365|      8783| Alyssa Richard|           3.0|2021-02-03|\n",
      "|    8365|       358| Alyssa Richard|           4.5|2021-02-03|\n",
      "|    8365|     23073| Alyssa Richard|           7.5|2021-02-03|\n",
      "|    8365|      9362| Alyssa Richard|           6.0|2021-02-03|\n",
      "|    8365|     34796| Alyssa Richard|           4.5|2021-02-03|\n",
      "|    8365|      3147| Alyssa Richard|           6.0|2021-02-03|\n",
      "|    8379|     13142|  Rebecca Kelly|           3.0|2021-02-03|\n",
      "|    8379|      9838|  Rebecca Kelly|           3.0|2021-02-03|\n",
      "|    8395|     41555|Amanda Mckinney|           6.0|2021-02-03|\n",
      "|    8395|     49039|Amanda Mckinney|           7.5|2021-02-03|\n",
      "|    8395|       394|Amanda Mckinney|           6.0|2021-02-03|\n",
      "|    8395|     28284|Amanda Mckinney|           7.5|2021-02-03|\n",
      "|    8395|     49599|Amanda Mckinney|           4.5|2021-02-03|\n",
      "|    8395|     47664|Amanda Mckinney|           4.5|2021-02-03|\n",
      "|    8395|     37171|Amanda Mckinney|           3.0|2021-02-03|\n",
      "|    8494|      7397|     Ann Nelson|           6.0|2021-02-04|\n",
      "|    8494|     37889|     Ann Nelson|           7.5|2021-02-04|\n",
      "+--------+----------+---------------+--------------+----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:58:52 WARN scheduler.TaskSetManager: Stage 64 contains a task of very large size (1111 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM users_notify_stream\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3ec0e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   13932|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:59:10 WARN scheduler.TaskSetManager: Stage 65 contains a task of very large size (1111 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM users_notify_stream\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8da521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_notify.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0d78e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_notify_delta = users_notify_df.writeStream\\\n",
    "#    .format('delta')\\\n",
    "#    .outputMode(\"append\")\\\n",
    "#    .option(\"checkpointlocation\", \"/tmp/tmp/checkpoint\")\\\n",
    "#    .start(\"/tmp/tmp/16_delta_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6839ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sql\n",
    "# CREATE TABLE tmp.users_notify\n",
    "#    USING DELTA\n",
    "#    LOCATION '/tmp/tmp/16_delta_stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e74fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select * from tmp.users_notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e83a2145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 15:59:16 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:59:17 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:59:18 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:59:19 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:59:19 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:59:20 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:59:20 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:59:21 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n",
      "22/04/03 15:59:21 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: hdfs://master:9000/datalake/bronze/dshop/clients/2022-03-04/clients.csv\n"
     ]
    }
   ],
   "source": [
    "users_notify_parquet = users_notify_df.writeStream\\\n",
    "    .format('parquet')\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"checkpointlocation\", \"/test/checkpoint\")\\\n",
    "    .start(\"/test/16_delta_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b510fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_notify_parquet.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8cccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
