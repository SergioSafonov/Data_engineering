{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f13d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4a7364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/05 15:42:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"lesson_15\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713a372b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.my_squared(s)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_squared(s):\n",
    "    return s * s + 1\n",
    "\n",
    "spark.udf.register(\"my_squared\", my_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5de88fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.range(1, 1001).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87fbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1, 1001).createOrReplaceTempView(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8545e7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|my_squared(id)|\n",
      "+---+--------------+\n",
      "|  1|             2|\n",
      "|  2|             5|\n",
      "|  3|            10|\n",
      "|  4|            17|\n",
      "|  5|            26|\n",
      "|  6|            37|\n",
      "|  7|            50|\n",
      "|  8|            65|\n",
      "|  9|            82|\n",
      "| 10|           101|\n",
      "| 11|           122|\n",
      "| 12|           145|\n",
      "| 13|           170|\n",
      "| 14|           197|\n",
      "| 15|           226|\n",
      "| 16|           257|\n",
      "| 17|           290|\n",
      "| 18|           325|\n",
      "| 19|           362|\n",
      "| 20|           401|\n",
      "+---+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT id, my_squared(id) from test\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90269d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dac730c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|((id * id) + 1)|\n",
      "+---+---------------+\n",
      "|  1|              2|\n",
      "|  2|              5|\n",
      "|  3|             10|\n",
      "|  4|             17|\n",
      "|  5|             26|\n",
      "|  6|             37|\n",
      "|  7|             50|\n",
      "|  8|             65|\n",
      "|  9|             82|\n",
      "| 10|            101|\n",
      "| 11|            122|\n",
      "| 12|            145|\n",
      "| 13|            170|\n",
      "| 14|            197|\n",
      "| 15|            226|\n",
      "| 16|            257|\n",
      "| 17|            290|\n",
      "| 18|            325|\n",
      "| 19|            362|\n",
      "| 20|            401|\n",
      "+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 49882)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/user/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/user/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/home/user/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/user/.local/lib/python3.8/site-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df.select('id', my_squared(df['id'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a186ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
